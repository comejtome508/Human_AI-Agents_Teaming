# Chapter 02 Understanding Foundation Models

* This chapter begins with how model developers curate training data, focusing on the distribution of training data.
* This chapter will address these questions 
    - what makes the transformer architecture so special that it continues to dominate? 
    - How long until another architecture takes over?
    - what might this new architecture look like?
* This chapter will also explore how a model developer might determine the appropriate size for their model.

* These will be discussed.
    - What exactly is human preference? 
    - How can it be represented in a way that a model can learn?
    - The way a model developer aligns their model has a significant impact on the model's usability.

* Not only does sampling explain many seemingly baffling AI behaviors, including hallucinations and inconsistencies, but choosing the right sampling strategy can also significantly boost a model's performance with relatively little effort.

---

## Training Data
* Training on more data often requires more compute resources and doesn't always lead to better performance.
* Depending on languages, the model's performance can be very different.
* Depending on the data, some models can be domain-specific. 

## Modeling
* Model Architecture
    - About Transformer Architecture
    


## Post-Training

## Sampling