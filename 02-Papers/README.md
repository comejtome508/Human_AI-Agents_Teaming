## Papers
||Title|Summary|Detail|Link|
|-|---|---|---|---|
|1|Collective eXplainable AI: Explaining Cooperative Strategies and Agent Contribution in Multiagent Reinforcement Learning with Shapley Values|- This paper introduces Collective eXplainable AI (XAI) using Shapley values to explain cooperative strategies in multi-agent reinforcement learning (RL). <br> <br>- The authors apply Monte Carlo approximations of Shapley values in multi-agent environments (Multiagent Particle and Sequential Social Dilemmas) to evaluate individual agent contributions to the global reward, demonstrating their effectiveness in explaining emergent behaviors.<br><br>- The study highlights Shapley values' limitations, such as their inability to explain specific episodes or actions, and suggests future research on computational efficiency and temporal contribution analysis in RL. |[Detail]()|[Link](https://www.semanticscholar.org/paper/Collective-eXplainable-AI%3A-Explaining-Cooperative-Heuillet-Couthouis/dd5c6cf926a257cf8d74506f08c5797f902bf863?utm_source=direct_link)|
|2|Human-Robot Alignment through Interactivity and Interpretability: Don’t Assume a “Spherical Human”|- Interactive and interpretable robot learning improves human-robot alignment by adapting to diverse human feedback.<br> <br>- Techniques like inverse reinforcement learning and explainable AI help robots learn from suboptimal human demonstrations. <br> <br>- Challenges include scaling teamwork, addressing AI biases, and ensuring ethical deployment. |[Detail]()| [Link](https://www.ijcai.org/proceedings/2024/976)|