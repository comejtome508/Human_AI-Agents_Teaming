## Papers
||Title|Summary|Detail|Link|
|-|---|---|---|---|
|1|Collective eXplainable AI: Explaining Cooperative Strategies and Agent Contribution in Multiagent Reinforcement Learning with Shapley Values|- This paper introduces Collective eXplainable AI (XAI) using Shapley values to explain cooperative strategies in multi-agent reinforcement learning (RL). <br> <br>- The authors apply Monte Carlo approximations of Shapley values in multi-agent environments (Multiagent Particle and Sequential Social Dilemmas) to evaluate individual agent contributions to the global reward, demonstrating their effectiveness in explaining emergent behaviors.<br><br>- The study highlights Shapley values' limitations, such as their inability to explain specific episodes or actions, and suggests future research on computational efficiency and temporal contribution analysis in RL. |[Detail]()|[Link](https://www.semanticscholar.org/paper/Collective-eXplainable-AI%3A-Explaining-Cooperative-Heuillet-Couthouis/dd5c6cf926a257cf8d74506f08c5797f902bf863?utm_source=direct_link)|
|2|Human-Robot Alignment through Interactivity and Interpretability: Don’t Assume a “Spherical Human”|- Interactive and interpretable robot learning improves human-robot alignment by adapting to diverse human feedback.<br> <br>- Techniques like inverse reinforcement learning and explainable AI help robots learn from suboptimal human demonstrations. <br> <br>- Challenges include scaling teamwork, addressing AI biases, and ensuring ethical deployment. |[Detail]()| [Link](https://www.ijcai.org/proceedings/2024/976)|
|2|Reciprocal MIND MELD- Improving Learning From Demonstration via Personalized, Reciprocal Teaching|- This paper introduces Reciprocal MIND MELD, a framework where robots help humans become better teachers by providing personalized, real-time feedback based on their teaching tendencies.<br> <br>- It uses a learned embedding of human suboptimality (like overcorrection or delayed feedback) and an LSTM-based Embedding Predictor Network (EPN) to dynamically update this profile without repeating calibration tasks. <br> <br>- Human studies in a driving simulator show that this approach improves both human teaching quality and robot learning outcomes on new tasks. |[Detail]()| [Link](https://proceedings.mlr.press/v205/schrum23a/schrum23a.pdf)|